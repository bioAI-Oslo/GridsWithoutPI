{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss: -0.003607:   4%|▎         | 1817/50000 [00:45<19:52, 40.40it/s][E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "[E thread_pool.cpp:109] Exception in thread pool task: mutex lock failed: Invalid argument\n",
      "loss: -0.003607:   4%|▎         | 1817/50000 [00:45<19:59, 40.18it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 25\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m progress: \u001b[38;5;66;03m# train loop\u001b[39;00m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# r = torch.rand((bs, 2), device = device)*4*np.pi - 2*np.pi\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# loss = model.train_step(inputs = r, labels = r, optimizer = optimizer)\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     r, v \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mgenerate_data(bs, \u001b[38;5;241m5\u001b[39m, device)\n\u001b[0;32m---> 25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     28\u001b[0m         loss_history\u001b[38;5;241m.\u001b[39mappend(loss)\n",
      "File \u001b[0;32m~/Documents/jobb/phd/SimGC/models.py:70\u001b[0m, in \u001b[0;36mFFGC.train_step\u001b[0;34m(self, inputs, labels, optimizer)\u001b[0m\n\u001b[1;32m     68\u001b[0m capacity_loss \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha)\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcapacity_loss(gs)\n\u001b[1;32m     69\u001b[0m loss \u001b[38;5;241m=\u001b[39m similarity_loss \u001b[38;5;241m+\u001b[39m capacity_loss\n\u001b[0;32m---> 70\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_loss_history\u001b[38;5;241m.\u001b[39mappend(similarity_loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from models import FFGC, RNNGC\n",
    "from dataset import DatasetMaker\n",
    "\n",
    "dataset = DatasetMaker()\n",
    "\n",
    "train_steps = 50000\n",
    "\n",
    "ng = 128\n",
    "bs = 256 # batch size \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model = FFGC(ng = ng, alpha=0.9)\n",
    "model = RNNGC(ng = ng, alpha=0.9)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_history = []\n",
    "progress = tqdm(range(train_steps))\n",
    "for i in progress: # train loop\n",
    "    # r = torch.rand((bs, 2), device = device)*4*np.pi - 2*np.pi\n",
    "    # loss = model.train_step(inputs = r, labels = r, optimizer = optimizer)\n",
    "    \n",
    "    r, v = dataset.generate_data(bs, 5, device)\n",
    "    loss = model.train_step(inputs = (r[:,0], v), labels = r, optimizer = optimizer)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        loss_history.append(loss)\n",
    "        progress.set_description(f\"loss: {loss:>7f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate on nxn grid\n",
    "model.to('cpu')\n",
    "n = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.linspace(-1, 1, n)*2*np.pi\n",
    "# y = x.copy()\n",
    "# xx, yy = np.meshgrid(x,y)\n",
    "# u = torch.tensor(np.stack([xx.ravel(), yy.ravel()], axis = -1), dtype= torch.float32)\n",
    "# p = model(u).detach().numpy()\n",
    "# p.shape # 1024, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, v = dataset.generate_data(10000, 5, device)\n",
    "g = model((r[:,0], v))\n",
    "\n",
    "r = r.detach().cpu().numpy()\n",
    "g = g.detach().cpu().numpy()\n",
    "p = scipy.stats.binned_statistic_2d(r[...,0].ravel(), r[...,1].ravel(), g.reshape(-1, g.shape[-1]).T, bins = 32)[0]\n",
    "p = p.reshape(ng, -1).T\n",
    "p.shape # 1024, 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = 10\n",
    "fig, ax = plt.subplots(n_p, n_p, figsize =(12, 12))\n",
    "\n",
    "for i, representation in enumerate(p.T[:n_p**2]):\n",
    "\n",
    "    row = i // n_p\n",
    "    col = i % n_p\n",
    "    ax[row, col].axis(\"off\")\n",
    "\n",
    "    representation = representation.reshape(n, n)\n",
    "\n",
    "    ax[row, col].imshow(representation, cmap = \"jet\", interpolation = \"none\")\n",
    "\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assume p is a torch tensor\n",
    "p0 = torch.tensor(p.astype(\"float32\").T)\n",
    "w0 = torch.nn.Parameter((torch.rand((ng, ng), dtype=torch.float32) * 2 - 1)*0.001)\n",
    "# \n",
    "\n",
    "# create a torch optimizer\n",
    "optimizer = torch.optim.Adam([w0], lr=1e-4)\n",
    "relu = torch.nn.ReLU()\n",
    "losses = []\n",
    "steps = 200000\n",
    "# define a training loop\n",
    "for _ in tqdm(range(steps)):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # z = Wp  # rotated version of population vector\n",
    "    z = w0@torch.tensor(p0)\n",
    "    a = w0.T@w0 - torch.eye(len(w0)) # be orthogonal\n",
    "    b = w0@w0.T - torch.eye(len(w0)) # be orthogonal\n",
    "    c = (torch.linalg.det(w0) - 1) # proper rotation\n",
    "    d = relu(-z) # non-negative result everywhere\n",
    "    \n",
    "    loss = torch.mean(a**2) + torch.mean(b**2) + torch.mean(c**2) + torch.mean(d) \n",
    "    losses.append(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogy(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def inf_rotate(v0, J, theta, n):\n",
    "#     I = np.eye(len(v0))\n",
    "#     R = I + theta*J\n",
    "#     v = v0.copy()\n",
    "#     for i in range(n):\n",
    "#         v = R@v # infinitesimal rotation\n",
    "#     return v\n",
    "\n",
    "# def random_skew_symmetric_matrix(n):\n",
    "#     J = np.random.choice([0, 1], (n, n))\n",
    "#     for i in range(n):\n",
    "#         for j in range(n):\n",
    "#             if i == j:\n",
    "#                 J[i,j] = 0\n",
    "#             elif j < i:\n",
    "#                 J[i,j] = -J[j,i]\n",
    "#     return J\n",
    "\n",
    "# j0 = random_skew_symmetric_matrix(len(p.T))\n",
    "# z = inf_rotate(p.T, j0, 1e-5, 50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = w0.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"Determinant: {np.linalg.det(w)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w@w.T) # orthogonality?\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = (w0@torch.tensor(p.T)).detach().numpy() # rotate population by W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = 10\n",
    "fig, ax = plt.subplots(n_p, n_p, figsize =(12, 12))\n",
    "\n",
    "for i, representation in enumerate(p.T[:n_p**2]):\n",
    "\n",
    "    row = i // n_p\n",
    "    col = i % n_p\n",
    "    ax[row, col].axis(\"off\")\n",
    "\n",
    "    representation = representation.reshape(n, n)\n",
    "\n",
    "    ax[row, col].imshow(representation, interpolation = \"none\")\n",
    "\n",
    "plt.suptitle(\"Before Rotation\")\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "\n",
    "n_p = 10\n",
    "fig, ax = plt.subplots(n_p, n_p, figsize =(10, 10))\n",
    "\n",
    "for i, representation in enumerate(z[:n_p**2]):\n",
    "\n",
    "    row = i // n_p\n",
    "    col = i % n_p\n",
    "    ax[row, col].axis(\"off\")\n",
    "\n",
    "    representation = representation.reshape(n, n)\n",
    "    ax[row, col].imshow(representation, interpolation = \"none\")\n",
    "\n",
    "plt.suptitle(\"After Rotation\")\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = z[:,512+16]\n",
    "sim = np.exp(-np.sum((ps[None] - z.T)**2, axis = -1))\n",
    "plt.imshow(sim.reshape(32,32), interpolation = \"None\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(20, 2, figsize = (2, 10))\n",
    "\n",
    "for i in range(20):\n",
    "    # ax[i,1].imshow(z[i].reshape(32,32),vmax = np.amax(p[:,i]))\n",
    "    ax[i,1].imshow(z[i].reshape(32,32),vmax = np.amax(p[:,i]))\n",
    "\n",
    "    ax[i, 0].imshow(p[:,i].reshape(32,32))\n",
    "    ax[i,0].axis(\"off\")\n",
    "    ax[i,1].axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_p = 10\n",
    "fig, ax = plt.subplots(n_p, n_p, figsize =(10, 10))\n",
    "\n",
    "for i, representation in enumerate(((p.T - z)**2)[:n_p**2]):\n",
    "\n",
    "    row = i // n_p\n",
    "    col = i % n_p\n",
    "    ax[row, col].axis(\"off\")\n",
    "\n",
    "    representation = representation.reshape(n, n)\n",
    "\n",
    "    ax[row, col].imshow(representation, interpolation = \"none\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(w)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
