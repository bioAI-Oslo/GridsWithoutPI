{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import spatial_maps as sm\n",
    "from scipy.integrate import cumtrapz\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models import FFGC, RNNGC\n",
    "from plotting_functions import *\n",
    "from dataset import DatasetMaker\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training parameters\n",
    "train_steps = 50000\n",
    "timesteps = 20\n",
    "bs = 128 # batch size\n",
    "ng = 256 # no. of recurrent units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get even distribution from distribution function\n",
    "\n",
    "def inverse_transform_sampling(prob_density_func, num_samples):\n",
    "    # Create an array across the interval [0, 1]\n",
    "    x = np.linspace(0.001, 0.999, 10000)\n",
    "\n",
    "    # Calculate the cumulative distribution function (CDF)\n",
    "    cdf = cumtrapz(prob_density_func(x), x)\n",
    "    cdf = np.insert(cdf, 0, 0)\n",
    "\n",
    "    # Use the CDF to get the inverse CDF\n",
    "    inverse_cdf = np.interp(np.linspace(0, cdf[-1], num_samples), cdf, x)\n",
    "\n",
    "    return inverse_cdf\n",
    "\n",
    "# alphas = inverse_transform_sampling(lambda x: 1/(x*(1-x)), 100)\n",
    "alphas = inverse_transform_sampling(lambda x: (1-x)/x + x/(1-x), 51)\n",
    "plt.hist(alphas, bins = 60)\n",
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FF Grid search\n",
    "\n",
    "alphas = inverse_transform_sampling(lambda x: (1-x)/x + x/(1-x), 51)\n",
    "\n",
    "# alphas = np.linspace(0.85, 0.94, 100)\n",
    "# alphas = 1-np.geomspace(0.001,1,30)\n",
    "grid_scores = []\n",
    "for alpha in tqdm(alphas):\n",
    "    print(f'Alpha: {alpha}')\n",
    "    # model = FFGC(alpha = alpha)\n",
    "    model = RNNGC(ng = ng, alpha = alpha, norm = \"l2\")\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "    # Train model\n",
    "    for i in range(train_steps-len(model.total_loss_history)):\n",
    "        r = torch.rand((bs, 2), device = device)*4*np.pi - 2*np.pi\n",
    "        loss = model.train_step(inputs = r, labels = r, optimizer = optimizer)\n",
    "\n",
    "    # Plot loss history\n",
    "    # fig,axs = plt.subplots(1,3,figsize=(9,3))\n",
    "    # axs[0].semilogy(np.array(model.total_loss_history)-model.loss_minima(), label = 'Total loss', color = 'black')\n",
    "    # axs[1].semilogy(np.array(model.capacity_loss_history)-model.loss_minima(), label = 'Capacity loss', color = 'red')\n",
    "    # axs[2].semilogy(model.similarity_loss_history, label = 'Similarity loss', color = 'blue')\n",
    "    # fig.tight_layout()\n",
    "    # fig.legend()\n",
    "    # plt.savefig(f'./gridsearch/ffgc_loss_alpha_{alpha}.png')\n",
    "    # plt.close()\n",
    "\n",
    "    # evaluate on 64x64 grid\n",
    "    res = 64\n",
    "    x = np.linspace(-1, 1, res)*2*np.pi\n",
    "    y = np.linspace(-1, 1, res)*2*np.pi\n",
    "    xx, yy = np.meshgrid(x,y)\n",
    "    r = np.stack([xx.ravel(), yy.ravel()], axis = -1)\n",
    "    gs = model(torch.tensor(r.astype(\"float32\"),device = device))\n",
    "    gs = gs.detach().cpu().numpy()\n",
    "    gs = gs.T.reshape(-1,res,res)\n",
    "\n",
    "    # plot spatial map\n",
    "    # multiimshow(gs[:25], figsize=(10,10), normalize=False);\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f'./gridsearch/ffgc_spatial_map_alpha_{alpha}.png')\n",
    "    # plt.close()\n",
    "\n",
    "    # Gridscore\n",
    "    grid_score = np.array([sm.gridness(gs[i]) for i in range(len(gs))])\n",
    "    grid_scores.append(grid_score)\n",
    "\n",
    "# Save grid scores\n",
    "grid_scores = np.array(grid_scores)\n",
    "np.save('./gridsearch/ffgc_grid_scores.npy', grid_scores)\n",
    "np.save('./gridsearch/ffgc_grid_scores_alphas.npy', alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Gridsearch\n",
    "\n",
    "dataset = DatasetMaker()\n",
    "\n",
    "for norm in [\"l1\", \"l2\"]:\n",
    "    grid_scores = []\n",
    "    for alpha in alphas:\n",
    "        print(f'Alpha: {alpha}')\n",
    "        model = RNNGC(ng = ng, alpha = alpha, norm = norm)\n",
    "        model.to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "        # Train model\n",
    "        for i in tqdm(range(train_steps-len(model.total_loss_history))):\n",
    "            r, v = dataset.generate_data(bs, timesteps, device)\n",
    "            loss = model.train_step(inputs = (r[:,0], v), labels = r, optimizer = optimizer)\n",
    "\n",
    "        # evaluate on 64x64 grid\n",
    "        r, v = dataset.generate_data(20000, 10, device)\n",
    "        g = model((r[:,0], v))\n",
    "\n",
    "        r = r.detach().cpu().numpy()\n",
    "        g = g.detach().cpu().numpy()\n",
    "        ratemaps = scipy.stats.binned_statistic_2d(r[...,0].ravel(), r[...,1].ravel(), g.reshape(-1, g.shape[-1]).T, bins = 64)[0]\n",
    "\n",
    "        # Gridscore\n",
    "        grid_score = np.array([sm.gridness(ratemaps[i]) for i in range(len(ratemaps))])\n",
    "        grid_scores.append(grid_score)\n",
    "\n",
    "        model.save(path=f\"./saved-models/{model.__class__.__name__}_{alpha}_{norm}.pkl\")\n",
    "\n",
    "    # Save grid scores\n",
    "    grid_scores = np.array(grid_scores)\n",
    "    np.save(f'./gridsearch/rnngc_grid_scores_{norm}.npy', grid_scores)\n",
    "    np.save(f'./gridsearch/rnnc_grid_scores_alphas_{norm}.npy', alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs to zeors\n",
    "grid_scores[np.isnan(grid_scores)] = 0\n",
    "grid_scores_binned = []\n",
    "gs_modes = np.zeros(len(grid_scores))\n",
    "\n",
    "gs_means = np.mean(grid_scores, axis = 1)\n",
    "gs_meadians = np.median(grid_scores, axis = 1)\n",
    "# mode on 30 bins\n",
    "for i in range(len(grid_scores)):\n",
    "    grid_scores_binned.append(np.histogram(grid_scores[i], bins = 100))\n",
    "    gs_modes[i] = grid_scores_binned[i][1][np.argmax(grid_scores_binned[i][0])]\n",
    "plt.plot(alphas, gs_means, label = 'Mean')\n",
    "plt.plot(alphas, gs_meadians, label = 'Median')\n",
    "plt.plot(alphas, gs_modes, label = 'Mode')\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('Grid score')\n",
    "plt.legend()\n",
    "# plt.savefig('./gridsearch/ffgc_grid_scores.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_max = alphas[np.argmax(gs_modes)]\n",
    "mean_max = alphas[np.argmax(gs_means)]\n",
    "median_max = alphas[np.argmax(gs_meadians)]\n",
    "print(f'Mode max: {mode_max}')\n",
    "print(f'Mean max: {mean_max}')\n",
    "print(f'Median max: {median_max}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
